{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df5d32-21dd-434e-a858-1a7755e0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e444c0-cf38-4930-9b2f-e86a60f671ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions_df = pd.read_csv(\"results/all_decisions_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed49eb-f064-42eb-88f2-4754a9565820",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_effort = []\n",
    "for dataset in all_decisions_df[\"dataset\"].unique():\n",
    "    _dataset_df = all_decisions_df.query(\"dataset == @dataset\").copy()\n",
    "    for task_scope in _dataset_df[\"task_scope\"].unique():\n",
    "        _task_scope_df = _dataset_df.query(\"task_scope == @task_scope\").copy()\n",
    "        for experiment_run in _task_scope_df[\"experiment_run\"].unique():\n",
    "            if task_scope == \"jaro\":\n",
    "                _experiment_df = _task_scope_df\n",
    "                experiment_run = 0\n",
    "            else:\n",
    "                _experiment_df = _task_scope_df.query(\"experiment_run == @experiment_run\").copy()\n",
    "            verification_effort.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"task_scope\": task_scope,\n",
    "                \"experiment_run\": experiment_run,\n",
    "                \"count\": _experiment_df.query(\"decision == 'yes'\").shape[0],\n",
    "                \"recall\": recall_score(\n",
    "                    _experiment_df[\"benchmark\"],\n",
    "                    _experiment_df[\"decision\"] == \"yes\",\n",
    "                    average=\"binary\",\n",
    "                    pos_label=True,\n",
    "                    zero_division=0.0,\n",
    "                )\n",
    "            })\n",
    "\n",
    "verification_effort_df = pd.DataFrame(verification_effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c62c4-cbe0-4375-8cdd-a210e3714b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "effort_and_recall_df = verification_effort_df.groupby(\n",
    "    by=[\"dataset\", \"task_scope\"]\n",
    ")[\"count\"].median().reset_index().merge(\n",
    "    verification_effort_df.groupby(\n",
    "        [\"dataset\", \"task_scope\", \"count\"]\n",
    "    )[\"recall\"].median().reset_index(),\n",
    "    on=[\"dataset\", \"task_scope\", \"count\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "median_efforts = pd.pivot(\n",
    "    effort_and_recall_df,\n",
    "    index=\"dataset\",\n",
    "    columns=\"task_scope\",\n",
    "    values=[\"count\", \"recall\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7869ae-630d-441c-8954-f2104ea7c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "scope_ord = [\"jaro\", \"1-to-1\", \"1-to-n\", \"n-to-1\", \"n-to-n\"]\n",
    "median_efforts[\"count\"] = median_efforts[\"count\"].astype(int)\n",
    "median_efforts = median_efforts.swaplevel(0, 1, axis=\"columns\")[itertools.product(scope_ord, (\"count\", \"recall\"))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba0359-ad2c-46a1-9a1c-35bc97e51b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def highlight_max_recall(s, props=''):\n",
    "    return np.where(\n",
    "        ([False, True] * 5) &  # ignore every other column (the count)\n",
    "        (s == np.nanmax(s.loc[(slice(None), \"recall\")].values)),\n",
    "        props,\n",
    "        ''\n",
    "    )\n",
    "\n",
    "median_efforts.style.apply(\n",
    "    highlight_max_recall,\n",
    "    props=\"font-weight: bold\",\n",
    "    axis=\"columns\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae43d3-1d89-4ab8-a58a-aaf53d5276b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def repeat_each(iterable, n):\n",
    "    \"Returns each sequence element n times.\"\n",
    "    return itertools.chain.from_iterable(itertools.repeat(e, n) for e in iterable)\n",
    "\n",
    "values = [[0 if i % 2 == 0 else value for i, value in enumerate(row)] for row in median_efforts.sort_index(ascending=False).values]\n",
    "texts = [[f\"{int(value):d}\" if i % 2 == 0 else f\"{value:.3f}\" for i, value in enumerate(row)] for row in median_efforts.sort_index(ascending=False).values]\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        x=[list(repeat_each([\"baseline\", \"1-to-1\", \"1-to-N\", \"N-to-1\", \"N-to-M\"], 2)), [\"count\", \"recall\"] * 5],\n",
    "        y=median_efforts.sort_index(ascending=False).index,\n",
    "        z=values,\n",
    "        text=texts,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16},\n",
    "        colorscale=\"BuGn\",\n",
    "        showscale=False,\n",
    "    ),\n",
    "    layout=dict(\n",
    "        title=\"Counts of matches reported per task scope and their recall\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd701d6-f17c-40d5-aca1-59bf37cd3610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
